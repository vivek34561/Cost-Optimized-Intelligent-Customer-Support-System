# RAG Chatbot with LangGraph & FAISS

## ğŸ“ Project Structure

```
Customer Support Chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                      # Configuration
â”‚   â”œâ”€â”€ faiss_index_builder.py         # Build FAISS index
â”‚   â”œâ”€â”€ retriever.py                   # RAG retriever
â”‚   â”œâ”€â”€ state/                         # State definitions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ state.py                   # ChatbotState TypedDict
â”‚   â”œâ”€â”€ llm/                           # LLM configurations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py                  # Groq LLM setup
â”‚   â”‚   â””â”€â”€ prompts.py                 # System prompts
â”‚   â”œâ”€â”€ nodes/                         # LangGraph nodes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ intent_node.py             # Intent classification
â”‚   â”‚   â”œâ”€â”€ retrieve_node.py           # RAG retrieval
â”‚   â”‚   â””â”€â”€ generate_node.py           # LLM generation
â”‚   â”œâ”€â”€ graph/                         # State machine
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ chatbot_graph.py           # LangGraph workflow
â”‚   â””â”€â”€ main.py                        # Main chatbot interface
â”œâ”€â”€ models/                            # Intent classification models
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl
â”‚   â”œâ”€â”€ logistic_regression_model.pkl
â”‚   â””â”€â”€ routing_config.json
â”œâ”€â”€ data/                              # FAISS index (created on build)
â”‚   â”œâ”€â”€ faiss_index
â”‚   â””â”€â”€ faiss_metadata.json
â”œâ”€â”€ intent_router.py                   # Intent routing module
â”œâ”€â”€ build_rag_index.py                 # Script to build FAISS index
â”œâ”€â”€ requirements-rag.txt               # RAG dependencies
â””â”€â”€ .env                              # API keys (create from .env.example)
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements-rag.txt
```

**Key Dependencies:**
- `sentence-transformers` - HuggingFace embeddings (free, runs locally)
- `faiss-cpu` - Local vector database (no cloud service needed)
- `groq` - Fast LLM inference (free tier available)
- `langgraph` - State machine orchestration

### 2. Set Up API Keys

Create `.env` file from `.env.example`:

```bash
cp .env.example .env
```

Edit `.env` and add your Groq API key:
```env
# Get free API key from: https://console.groq.com
GROQ_API_KEY=your_groq_api_key_here

# Optional: Override default models
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
LLM_MODEL=llama-3.3-70b-versatile
```

### 3. Build FAISS Index

**Option A: Test with 100 documents (recommended first)**
```bash
python build_rag_index.py --limit 100
```

**Option B: Full index (26,872 documents)**
```bash
python build_rag_index.py
```

The embedding model (all-MiniLM-L6-v2, ~90MB) will download automatically on first run.

### 4. Test the Chatbot

**Run test queries:**
```bash
python src/main.py
```

**Interactive mode:**
```bash
python src/main.py interactive
```

## ğŸ—ï¸ Architecture

### Embedding Model (HuggingFace)
- **Model:** `sentence-transformers/all-MiniLM-L6-v2`
- **Dimension:** 384
- **Advantages:**
  - âœ… Free, runs locally
  - âœ… No API calls needed
  - âœ… Fast inference (~1ms per query)
  - âœ… Good quality for semantic search

### Vector Database (FAISS)
- **Type:** IndexFlatIP (cosine similarity via normalized vectors)
- **Storage:** Local files (`data/faiss_index` + `data/faiss_metadata.json`)
- **Advantages:**
  - âœ… No cloud service needed
  - âœ… No API keys for vector DB
  - âœ… Fast for small-medium datasets
  - âœ… Can version control the index

### LLM (Groq)
- **Model:** `llama-3.3-70b-versatile`
- **Advantages:**
  - âœ… Ultra-fast inference (up to 750 tokens/sec)
  - âœ… Free tier available
  - âœ… Low latency

### LangGraph State Machine

```
START â†’ intent_node â†’ [conditional routing] â†’ generate_node â†’ END
                              â†“
                        retrieve_node (only for BUCKET_B)
                              â†“
                        generate_node â†’ END
```

**Flow:**
1. **intent_node**: Classifies intent and determines bucket (A/B/C)
2. **Conditional routing**: 
   - BUCKET_A (FAQ) â†’ skip retrieval, go to generate
   - BUCKET_B (RAG) â†’ retrieve from FAISS â†’ generate
   - BUCKET_C (Escalation) â†’ skip retrieval, go to generate
3. **retrieve_node**: Query FAISS index for relevant documents
4. **generate_node**: Generate response using Groq LLM with context

## ğŸ¯ Three-Bucket Routing System

### BUCKET_A: Zero-Cost (Direct Responses)
**Intents:** check_invoice, check_payment_methods, track_order, delivery_options, check_refund_policy, check_cancellation_fee, delivery_period, track_refund

**Handling:** Template responses, no LLM needed

**Cost:** $0

### BUCKET_B: Low-Cost (RAG + Small LLM)
**Intents:** cancel_order, change_order, change_shipping_address, create_account, delete_account, edit_account, get_invoice, get_refund, newsletter_subscription, payment_issue, place_order, recover_password, registration_problems, review, set_up_shipping_address, switch_account

**Handling:** FAISS retrieval â†’ Groq LLM generation

**Cost:** ~$0.0001 per query

### BUCKET_C: High-Cost (Escalation)
**Intents:** complaint, payment_issue (low confidence), contact_human_agent, contact_customer_service

**Handling:** Escalation message or GPT-4 equivalent

**Cost:** Variable

## ğŸ“Š Performance Metrics

From dry-run evaluation (500 samples):
- **BUCKET_A (Zero-cost):** 30.6%
- **BUCKET_B (Low-cost):** 51.6%
- **BUCKET_C (High-cost):** 17.8%
- **Cost Savings:** 79.6% vs uniform big LLM approach

## ğŸ”§ Configuration

### Embedding Configuration (src/config.py)
```python
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
EMBEDDING_DIMENSION = 384
```

### RAG Configuration
```python
TOP_K_RETRIEVAL = 3  # Number of documents to retrieve
```

### LLM Configuration
```python
LLM_MODEL = "llama-3.3-70b-versatile"
LLM_TEMPERATURE = 0.7
LLM_MAX_TOKENS = 500
```

## ğŸ§ª Testing Components

### Test Retrieval
```bash
python src/retriever.py
```

### Test Full Graph
```bash
python src/main.py
```

### Interactive Chat
```bash
python src/main.py interactive
```

Example session:
```
ğŸ§‘ You: How do I track my order?
ğŸ“Š Routing: BUCKET_A | track_order (98%)
ğŸ¤– Bot: I'd be happy to help you track your order! Please provide your order number...

ğŸ§‘ You: How can I cancel my subscription?
ğŸ“Š Routing: BUCKET_B | cancel_order (95%)
ğŸ¤– Bot: [RAG-powered response based on knowledge base]

ğŸ§‘ You: I'm very unhappy with your service!
ğŸ“Š Routing: BUCKET_C | complaint (92%)
ğŸ¤– Bot: I understand you're experiencing an issue...
```

## ğŸš€ Next Steps

1. **FastAPI Deployment:** Create REST API endpoint
2. **Conversation Memory:** Add chat history tracking
3. **Streaming Responses:** Implement real-time streaming
4. **Monitoring:** Add logging and analytics
5. **Production Deployment:** Docker containerization

## ğŸ’¡ Tips

1. **First Run:** Start with `--limit 100` to test quickly
2. **Embedding Model:** Downloads once, cached thereafter (~90MB)
3. **FAISS Index:** Stored locally, can be committed to git if desired
4. **Groq API:** Free tier provides generous limits for testing
5. **No Internet:** Embeddings work offline after model download

## ğŸ› Troubleshooting

**FAISS index not found:**
```bash
python build_rag_index.py --limit 10
```

**Embedding model slow to download:**
- Model downloads from HuggingFace Hub (~90MB)
- Subsequent runs use cached model

**Groq API errors:**
- Check API key in `.env`
- Verify API quota at console.groq.com

**Import errors:**
```bash
pip install -r requirements-rag.txt --upgrade
```

## ğŸ“ License

See LICENSE file for details.
